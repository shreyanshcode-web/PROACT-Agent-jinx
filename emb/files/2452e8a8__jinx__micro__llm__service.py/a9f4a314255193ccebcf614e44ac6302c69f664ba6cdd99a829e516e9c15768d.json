{"meta": {"ts": 1760763312.2660708, "model": "text-embedding-3-small", "file_rel": "jinx\\micro\\llm\\service.py", "chunk_index": 9, "chunks_total": 11, "content_sha256": "a9f4a314255193ccebcf614e44ac6302c69f664ba6cdd99a829e516e9c15768d", "file_sha256": "78807d0373da774c2a370ab101965a658489ed517eb256c6a9865b726efefacc", "terms": ["str", "model", "async", "await", "stxt", "tag", "on_first_block", "prompt_override", "req_path", "none", "llm", "try", "with", "_asyncio", "code_id", "dump_task", "timing_section", "def", "except", "exception", "out", "tuple", "txt", "_prepare_request", "call_fallback"], "text_preview": "async def spark_openai_streaming(txt: str, *, prompt_override: str | None = None, on_first_block=None) -> tuple[str, str]:\n    \"\"\"Streaming LLM call with early execution on first complete <python_{tag}> block.\n\n    Returns (full_output_text, code_tag_id).\n", "dims": 0, "line_start": 232, "line_end": 259}, "embedding": []}