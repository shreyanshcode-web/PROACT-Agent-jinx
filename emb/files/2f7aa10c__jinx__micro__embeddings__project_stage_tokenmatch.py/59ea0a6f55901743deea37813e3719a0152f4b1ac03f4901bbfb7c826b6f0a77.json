{"meta": {"ts": 1760763311.5590038, "model": "text-embedding-3-small", "file_rel": "jinx\\micro\\embeddings\\project_stage_tokenmatch.py", "chunk_index": 0, "chunks_total": 5, "content_sha256": "59ea0a6f55901743deea37813e3719a0152f4b1ac03f4901bbfb7c826b6f0a77", "file_sha256": "67bc526c06f34ae39396a150265c9715c858ae420d85712b51fd431dd2b5c307", "terms": ["import", "int", "tuple", "from", "list", "return", "limit_ms", "str", "tokenize", "and", "def", "for", "none", "try", "_re", "data", "except", "not", "src", "time", "tok", "tokens", "__future__", "_name_re", "_stop"], "text_preview": "from __future__ import annotations\n\nimport io\nimport os\nimport time\nimport tokenize\nfrom typing import Any, Dict, List, Tuple\nimport re as _re\n\nfrom .project_config import ROOT, EXCLUDE_DIRS, MAX_FILE_BYTES\nfrom .project_iter import iter_candidate_files\nfr", "dims": 0, "line_start": 1, "line_end": 38}, "embedding": []}