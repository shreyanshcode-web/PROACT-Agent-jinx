{"meta": {"ts": 1760763310.8034267, "model": "text-embedding-3-small", "file_rel": "jinx\\micro\\embeddings\\embed_cache.py", "chunk_index": 0, "chunks_total": 6, "content_sha256": "eb2c3266b4b4ae081da92d82a19ccad5b5aa0e61d120cd33ade16d18cab2eabb", "file_sha256": "5687542652356ac9e9caeedf34ee15eb9545edb77bc11053b973daf301bafe83", "terms": ["import", "str", "from", "float", "tuple", "getenv", "_max_conc", "asyncio", "dict", "except", "exception", "jinx", "try", "_append", "_dump", "_timeout_ms", "blue_whispers", "for", "text", "cache", "def", "int", "line", "list", "__future__"], "text_preview": "from __future__ import annotations\n\nimport asyncio\nimport os\nimport time\nfrom typing import Any, Dict, List, Tuple\n\nfrom jinx.net import get_openai_client\n\n# Simple in-memory TTL cache with request coalescing and concurrency limiting\n# Keys are (model, tex", "dims": 0, "line_start": 1, "line_end": 41}, "embedding": []}