{"meta": {"ts": 1760763312.4889176, "model": "text-embedding-3-small", "file_rel": "jinx\\micro\\memory\\optimizer.py", "chunk_index": 0, "chunks_total": 10, "content_sha256": "2b39e937dd0041f43d11d5afd823cf2c30386ce2c3ad3e09af8905f8a83ee0ac", "file_sha256": "fd06aaa8fc7aec54936c600667e1093ceb46733b864a41391f780abce05d8d6a", "terms": ["jinx", "memory", "from", "micro", "import", "and", "state", "__future__", "all_tags", "bomb_log", "build_local_memory", "call_openai", "compact_weekly", "detonate_payload", "get_prompt", "glitch_pulse", "history_compactor", "ingest_memory", "jx_state", "local_builder", "log_paths", "logging_service", "openai_mod", "openai_requests", "openai_requests_dir_memory"], "text_preview": "from __future__ import annotations\n\n\"\"\"Memory optimization pipeline (micro-module).\n\nCollects recent transcript and evergreen memory, asks the LLM to compact\nand persist updated memory state, and serializes executions through a\nsingle worker to preserve or", "dims": 0, "line_start": 1, "line_end": 31}, "embedding": []}