{"file_rel": "jinx\\micro\\llm\\service.py", "file_sha256": "78807d0373da774c2a370ab101965a658489ed517eb256c6a9865b726efefacc", "updated_ts": 1760763312.367579, "total_chunks": 11, "chunks": [{"sha": "35911ccce5e01d821e1f5c616e235e5137aa5038459fe9fcd1048a39a5370e0e", "index": 0, "path": "2452e8a8__jinx__micro__llm__service.py\\35911ccce5e01d821e1f5c616e235e5137aa5038459fe9fcd1048a39a5370e0e.json", "terms": ["jinx", "from", "micro", "import", "str", "none", "__future__", "_dt", "_is_code_like", "build_header_and_tag", "call_openai", "call_openai_stream_first_block", "call_openai_validated", "code_primer", "code_tag_id", "compose_dynamic_prompt", "detonate_payload", "expand_dynamic_macros", "header_plus_prompt", "is_code_like", "list_programs", "load_last_anchors", "load_macro_plugins", "log_paths", "macro_plugins"], "text_preview": "from __future__ import annotations\n\nimport os\nfrom jinx.openai_mod import build_header_and_tag\nfrom .openai_caller import call_openai, call_openai_validated, call_openai_stream_first_block\nfrom jinx.log_paths import OPENAI_REQUESTS_DIR_GENERAL\nfrom jinx.lo", "line_start": 1, "line_end": 28}, {"sha": "162f9d16fc6598b941811395a2c0e4175a452105d0db58a3ea0cca4c94985f20", "index": 1, "path": "2452e8a8__jinx__micro__llm__service.py\\162f9d16fc6598b941811395a2c0e4175a452105d0db58a3ea0cca4c94985f20.json", "terms": ["str", "not", "try", "auto_on", "prompt_override", "await", "except", "exception", "false", "getenv", "lower", "off", "tag", "use_dlg", "none", "and", "macros", "return", "true", "_prepare_request", "build_header_and_tag", "code_primer", "compose_dynamic_prompt", "jinx_automacro_dialogue", "jinx_automacro_project"], "text_preview": "return await build_header_and_tag(prompt_override)\n\n\nasync def _prepare_request(txt: str, *, prompt_override: str | None = None) -> tuple[str, str, str, str, str]:\n    \"\"\"Compose instructions and return (jx, tag, model, sx, stxt).\"\"\"\n    jx, tag = await co", "line_start": 29, "line_end": 51}, {"sha": "017d903bfc94d0d5e84efb09d9853d9e6dce9ed86fc9ce6c1cad584fed59c6a2", "index": 2, "path": "2452e8a8__jinx__micro__llm__service.py\\017d903bfc94d0d5e84efb09d9853d9e6dce9ed86fc9ce6c1cad584fed59c6a2.json", "terms": ["except", "exception", "getenv", "try", "int", "dlg_k", "mem_comp_k", "mem_ever_k", "proj_k", "use_mem", "use_proj", "for", "codey", "not", "true", "_is_code_like", "jinx_automacro_dialogue_k", "jinx_automacro_mem_compact_k", "jinx_automacro_mem_evergreen_k", "jinx_automacro_memory", "jinx_automacro_project_k", "use_dlg", "allowed", "and", "automacros"], "text_preview": "use_proj = True\n            # Dynamic topK per source\n            try:\n                dlg_k = int(os.getenv(\"JINX_AUTOMACRO_DIALOGUE_K\", \"3\"))\n            except Exception:\n                dlg_k = 3\n            try:\n                proj_k = int(os.getenv(", "line_start": 52, "line_end": 78}, {"sha": "bde8b5e080e38d6f7406ecda0fc99c4eb0ede917151f6ea94262a62a1cb2d651", "index": 3, "path": "2452e8a8__jinx__micro__llm__service.py\\bde8b5e080e38d6f7406ecda0fc99c4eb0ede917151f6ea94262a62a1cb2d651.json", "terms": ["lines", "not", "dialogue", "export", "append", "include_patch", "context", "emb", "patch", "recent", "dlg_k", "codey", "empty", "may", "memory", "routed", "exp_lines", "jinx_automacro_patch_exports", "last_patch_", "last_patch_commit", "last_patch_preview", "mem_comp_k", "mem_ever_k", "proj_k", "use_dlg"], "text_preview": "lines.append(f\"Context (dialogue): {{{{m:emb:dialogue:{dlg_k}}}}}\")\n                elif not codey:\n                    lines.append(f\"Context (dialogue): {{{{m:emb:dialogue:{dlg_k}}}}}\")\n            if use_proj:\n                if codey or not use_dlg:\n  ", "line_start": 79, "line_end": 98}, {"sha": "7e935d596f84e10808ca95bb48258b217e424efb383ec4d4acebe5fdc9067a26", "index": 4, "path": "2452e8a8__jinx__micro__llm__service.py\\7e935d596f84e10808ca95bb48258b217e424efb383ec4d4acebe5fdc9067a26.json", "terms": ["export", "not", "verification", "include_verify", "include_run", "except", "exception", "false", "getenv", "include", "join", "last", "lower", "off", "optionally", "patch", "reason", "recent", "str", "true", "try", "vlines", "exp_lines", "jinx_automacro_run_exports", "jinx_automacro_verify_exports"], "text_preview": "\"Recent Patch Strategy: {{export:last_patch_strategy:1}}\",\n                \"Recent Patch Reason: {{export:last_patch_reason:1}}\",\n            ]\n            jx = jx + \"\\n\" + \"\\n\".join(exp_lines) + \"\\n\"\n        # Optionally include last verification results\n", "line_start": 99, "line_end": 119}, {"sha": "05d7a591a3fbab9bc8b98f688369359c35a64b5cea78a3e505add6fd514b4eea", "index": 5, "path": "2452e8a8__jinx__micro__llm__service.py\\05d7a591a3fbab9bc8b98f688369359c35a64b5cea78a3e505add6fd514b4eea.json", "terms": ["run", "run_chars", "anc", "except", "exception", "last", "progs", "try", "for", "getcwd", "status", "stderr", "stdout", "and", "await", "chars", "rlines", "arg1", "arg2", "include_run", "jinx_macro_mem_preview_chars", "list_programs", "load_last_anchors", "os_name", "py_ver"], "text_preview": "if include_run and (\"{{m:run:\" not in jx):\n            try:\n                run_chars = max(24, int(os.getenv(\"JINX_MACRO_MEM_PREVIEW_CHARS\", \"160\")))\n            except Exception:\n                run_chars = 160\n            rlines = [\n                f\"La", "line_start": 120, "line_end": 146}, {"sha": "d24a218c13bce8c97aa15f673068c2260731321f27eb09a8aba533dee931f8b3", "index": 6, "path": "2452e8a8__jinx__micro__llm__service.py\\d24a218c13bce8c97aa15f673068c2260731321f27eb09a8aba533dee931f8b3.json", "terms": ["_init_lock", "spark_openai", "_macro_inited", "except", "exception", "getattr", "try", "_asyncio", "_dt", "_macro_init_lock", "await", "datetime", "false", "int", "none", "not", "now", "pass", "providers", "setattr", "input_text", "jinx_prompt_macro_max", "load_macro_plugins", "max_exp", "now_epoch"], "text_preview": "now_iso=_dt.datetime.now().isoformat(timespec=\"seconds\"),\n            now_epoch=str(int(_dt.datetime.now().timestamp())),\n            input_text=txt or \"\",\n        )\n        # Ensure built-in providers and plugin macros are registered/loaded\n        # Init", "line_start": 147, "line_end": 172}, {"sha": "5bcdbfb88c612fccba66da845ac37189d04c1578e4c6e07ec1ddc7f54dbafb95", "index": 7, "path": "2452e8a8__jinx__micro__llm__service.py\\5bcdbfb88c612fccba66da845ac37189d04c1578e4c6e07ec1ddc7f54dbafb95.json", "terms": ["str", "txt", "prompt_override", "await", "model", "stxt", "tag", "_asyncio", "est_tokens", "max_exp", "sanitize_prompt_for_external_api", "len", "none", "async", "call", "def", "except", "exception", "pass", "return", "tuple", "with", "_prepare_request", "code_tag_id", "create_task"], "text_preview": "max_exp = 50\n        jx = await expand_dynamic_macros(jx, ctx, max_expansions=max_exp)\n        # Best-effort token hint (chars/4 heuristic) for dynamic memory budgets\n        try:\n            est_tokens = max(0, (len(jx) + len(txt or \"\")) // 4)\n           ", "line_start": 173, "line_end": 201}, {"sha": "9c26a75d60a4b2665130d05c1ef3f8f0785bf81bbb6d1c2814cc498e6b970b37", "index": 8, "path": "2452e8a8__jinx__micro__llm__service.py\\9c26a75d60a4b2665130d05c1ef3f8f0785bf81bbb6d1c2814cc498e6b970b37.json", "terms": ["await", "model", "out", "req_path", "except", "exception", "path", "stxt", "try", "timing_section", "async", "call", "general", "llm", "multi", "retries", "return", "sample", "tag", "with", "_asyncio", "call_legacy", "call_openai", "call_openai_validated", "code_id"], "text_preview": "target_dir=OPENAI_REQUESTS_DIR_GENERAL,\n            kind=\"GENERAL\",\n            instructions=sx,\n            input_text=stxt,\n            model=model,\n        ))\n        # Preferred: validated multi-sample path\n        try:\n            async with timing_se", "line_start": 202, "line_end": 231}, {"sha": "a9f4a314255193ccebcf614e44ac6302c69f664ba6cdd99a829e516e9c15768d", "index": 9, "path": "2452e8a8__jinx__micro__llm__service.py\\a9f4a314255193ccebcf614e44ac6302c69f664ba6cdd99a829e516e9c15768d.json", "terms": ["str", "model", "async", "await", "stxt", "tag", "on_first_block", "prompt_override", "req_path", "none", "llm", "try", "with", "_asyncio", "code_id", "dump_task", "timing_section", "def", "except", "exception", "out", "tuple", "txt", "_prepare_request", "call_fallback"], "text_preview": "async def spark_openai_streaming(txt: str, *, prompt_override: str | None = None, on_first_block=None) -> tuple[str, str]:\n    \"\"\"Streaming LLM call with early execution on first complete <python_{tag}> block.\n\n    Returns (full_output_text, code_tag_id).\n", "line_start": 232, "line_end": 259}, {"sha": "36e2bdfd8b3bb0ef3d0762f039e5b9736e8c5168b6166e3a1fe200f5efc2b938", "index": 10, "path": "2452e8a8__jinx__micro__llm__service.py\\36e2bdfd8b3bb0ef3d0762f039e5b9736e8c5168b6166e3a1fe200f5efc2b938.json", "terms": ["out", "return", "_asyncio", "create_task", "detonate_payload", "openai_task", "req_path", "write_openai_response_append", "await", "except", "exception", "general", "pass", "retries", "tag"], "text_preview": "_asyncio.create_task(write_openai_response_append(req_path, \"GENERAL\", out))\n        except Exception:\n            pass\n        return (out, tag)\n\n    return await detonate_payload(openai_task, retries=1)", "line_start": 260, "line_end": 265}], "file_terms": ["str", "except", "exception", "try", "import", "not", "await", "from", "tag", "getenv", "model", "none", "prompt_override", "export", "async", "stxt", "and", "jinx", "false", "return", "txt", "with", "_asyncio", "req_path", "run"]}