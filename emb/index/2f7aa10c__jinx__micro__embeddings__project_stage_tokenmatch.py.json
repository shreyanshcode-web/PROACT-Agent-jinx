{"file_rel": "jinx\\micro\\embeddings\\project_stage_tokenmatch.py", "file_sha256": "67bc526c06f34ae39396a150265c9715c858ae420d85712b51fd431dd2b5c307", "updated_ts": 1760763311.6233525, "total_chunks": 5, "chunks": [{"sha": "59ea0a6f55901743deea37813e3719a0152f4b1ac03f4901bbfb7c826b6f0a77", "index": 0, "path": "2f7aa10c__jinx__micro__embeddings__project_stage_tokenmatch.py\\59ea0a6f55901743deea37813e3719a0152f4b1ac03f4901bbfb7c826b6f0a77.json", "terms": ["import", "int", "tuple", "from", "list", "return", "limit_ms", "str", "tokenize", "and", "def", "for", "none", "try", "_re", "data", "except", "not", "src", "time", "tok", "tokens", "__future__", "_name_re", "_stop"], "text_preview": "from __future__ import annotations\n\nimport io\nimport os\nimport time\nimport tokenize\nfrom typing import Any, Dict, List, Tuple\nimport re as _re\n\nfrom .project_config import ROOT, EXCLUDE_DIRS, MAX_FILE_BYTES\nfrom .project_iter import iter_candidate_files\nfr", "line_start": 1, "line_end": 38}, {"sha": "e17aaa64ce9ff7c2b622eaddad32a2cf3a2375e2508121a76250fb9610de9908", "index": 1, "path": "2f7aa10c__jinx__micro__embeddings__project_stage_tokenmatch.py\\e17aaa64ce9ff7c2b622eaddad32a2cf3a2375e2508121a76250fb9610de9908.json", "terms": ["int", "tokenize", "tuple", "list", "str", "not", "continue", "return", "first", "needle", "out", "end_line", "start_line", "tok_pairs", "append", "def", "hay", "match", "none", "the", "tok", "tokens", "toks", "_col", "_match_ordered_subsequence"], "text_preview": "s = tok.string\n            if ttype in (tokenize.ENCODING, tokenize.NL, tokenize.NEWLINE, tokenize.INDENT, tokenize.DEDENT, tokenize.ENDMARKER, tokenize.COMMENT):\n                continue\n            if not s:\n                continue\n            toks.appe", "line_start": 39, "line_end": 73}, {"sha": "cb96e9e98d9156722c43011e0ee539d2d6220fd7cf4083185c44ebb18a5fbc49", "index": 2, "path": "2f7aa10c__jinx__micro__embeddings__project_stage_tokenmatch.py\\cb96e9e98d9156722c43011e0ee539d2d6220fd7cf4083185c44ebb18a5fbc49.json", "terms": ["str", "hay", "len", "return", "end_line", "and", "needle", "query", "start_line", "none", "q_sig", "q_toks", "q_vals", "agnostic", "int", "any", "def", "dict", "float", "for", "list", "not", "tuple", "while", "_ln"], "text_preview": "while hi < len(hay) and hay[hi][0] != needle[0]:\n        hi += 1\n    if hi >= len(hay):\n        return None\n    start_line = hay[hi][1]\n    # advance for the rest\n    ni = 1\n    hi += 1\n    while ni < len(needle) and hi < len(hay):\n        if hay[hi][0] ==", "line_start": 74, "line_end": 113}, {"sha": "dd7f75d388817dd098a2f70e0e46ef995f259a6c17439d79df90432deacdbc15", "index": 3, "path": "2f7aa10c__jinx__micro__embeddings__project_stage_tokenmatch.py\\dd7f75d388817dd098a2f70e0e46ef995f259a6c17439d79df90432deacdbc15.json", "terms": ["return", "lines", "false", "txt", "e_line", "s_line", "len", "span", "toks", "hay_sig", "rel_p", "set", "hits", "join", "max", "min", "not", "obj", "snip", "str", "strip", "try", "_match_ordered_subsequence", "_significant_tokens", "_time_up"], "text_preview": "if _time_up(t0, max_time_ms):\n            return True\n        try:\n            with open(abs_p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                txt = f.read()\n        except Exception:\n            return False\n        if not txt:\n            ", "line_start": 114, "line_end": 150}, {"sha": "7d1ac8a9352a850d3ea1fc42590a9f6f79196db38a99437ec72bd4901fbd1c99", "index": 4, "path": "2f7aa10c__jinx__micro__embeddings__project_stage_tokenmatch.py\\7d1ac8a9352a850d3ea1fc42590a9f6f79196db38a99437ec72bd4901fbd1c99.json", "terms": ["rel", "for", "hits", "return", "exclude_dirs", "max_file_bytes", "rel_files", "get", "obj", "pass", "process", "root", "seen", "__all__", "file_rel", "include_exts", "iter_candidate_files", "iter_project_chunks", "stage_tokenmatch_hits", "add", "and", "append", "except", "exception", "general"], "text_preview": "for fr, obj in iter_project_chunks():\n            rel = fr or str((obj.get(\"meta\") or {}).get(\"file_rel\") or \"\")\n            if rel and rel not in seen:\n                seen.add(rel)\n                rel_files.append(rel)\n        for rel in rel_files:\n     ", "line_start": 151, "line_end": 171}], "file_terms": ["return", "str", "int", "tuple", "list", "not", "tokenize", "rel", "import", "hay", "and", "for", "none", "len", "def", "needle", "end_line", "lines", "from", "hits", "toks", "start_line", "continue", "except", "false"]}