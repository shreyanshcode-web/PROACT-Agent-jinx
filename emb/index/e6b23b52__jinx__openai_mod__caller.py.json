{"file_rel": "jinx\\openai_mod\\caller.py", "file_sha256": "745fa7aa40a083eaf9ecfda9076a0922d7519f776e0fc7ece32aa45ac504c8fb", "updated_ts": 1760763313.551902, "total_chunks": 1, "chunks": [{"sha": "9aab0ca4d521be13ce570eb74059d5c893792571e0546287635458c42c2d27e0", "index": 0, "path": "e6b23b52__jinx__openai_mod__caller.py\\9aab0ca4d521be13ce570eb74059d5c893792571e0546287635458c42c2d27e0.json", "terms": ["str", "call_openai", "micro", "_call_openai", "input_text", "openai_caller", "api", "from", "import", "instructions", "jinx", "llm", "model", "return", "__future__", "and", "annotations", "async", "await", "call", "def", "delegate", "delegates", "facade", "implementation"], "text_preview": "from __future__ import annotations\n\n# Thin facade: delegate to micro-module implementation to keep API stable.\nfrom jinx.micro.llm.openai_caller import call_openai as _call_openai\n\n\nasync def call_openai(instructions: str, model: str, input_text: str) -> s", "line_start": 1, "line_end": 12}], "file_terms": ["str", "call_openai", "micro", "_call_openai", "input_text", "openai_caller", "api", "from", "import", "instructions", "jinx", "llm", "model", "return", "__future__", "and", "annotations", "async", "await", "call", "def", "delegate", "delegates", "facade", "implementation"]}