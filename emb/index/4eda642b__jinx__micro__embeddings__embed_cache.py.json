{"file_rel": "jinx\\micro\\embeddings\\embed_cache.py", "file_sha256": "5687542652356ac9e9caeedf34ee15eb9545edb77bc11053b973daf301bafe83", "updated_ts": 1760763310.913589, "total_chunks": 6, "chunks": [{"sha": "eb2c3266b4b4ae081da92d82a19ccad5b5aa0e61d120cd33ade16d18cab2eabb", "index": 0, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\eb2c3266b4b4ae081da92d82a19ccad5b5aa0e61d120cd33ade16d18cab2eabb.json", "terms": ["import", "str", "from", "float", "tuple", "getenv", "_max_conc", "asyncio", "dict", "except", "exception", "jinx", "try", "_append", "_dump", "_timeout_ms", "blue_whispers", "for", "text", "cache", "def", "int", "line", "list", "__future__"], "text_preview": "from __future__ import annotations\n\nimport asyncio\nimport os\nimport time\nfrom typing import Any, Dict, List, Tuple\n\nfrom jinx.net import get_openai_client\n\n# Simple in-memory TTL cache with request coalescing and concurrency limiting\n# Keys are (model, tex", "line_start": 1, "line_end": 41}, {"sha": "f41a1e37dab0b3f6985da461018f9e13371bf2ca62a19184cd2a215b39665b4e", "index": 1, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\f41a1e37dab0b3f6985da461018f9e13371bf2ca62a19184cd2a215b39665b4e.json", "terms": ["model", "str", "list", "return", "text", "vec", "none", "def", "async", "float", "_mem", "resp", "await", "_dump_line", "_now", "_sem", "_worker", "asyncio", "data", "len", "time", "client", "exp", "max", "not"], "text_preview": "return time.time()\n\n\ndef _cache_get(model: str, text: str) -> List[float] | None:\n    k = (model, text)\n    v = _mem.get(k)\n    if not v:\n        return None\n    exp, vec = v\n    if exp < _now():\n        _mem.pop(k, None)\n        return None\n    return vec", "line_start": 42, "line_end": 80}, {"sha": "404fd7e609ab52ea7a913752221096160c84349c20ed0ddf0f0ed68092a8b7b1", "index": 2, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\404fd7e609ab52ea7a913752221096160c84349c20ed0ddf0f0ed68092a8b7b1.json", "terms": ["model", "return", "list", "await", "texts", "data", "fut", "none", "not", "out", "try", "vec", "_dump_line", "_worker", "asyncio", "str", "batch", "client", "def", "except", "exception", "float", "for", "key", "len"], "text_preview": "await _dump_line(f\"call batch model={model} n={len(texts)}\")\n        def _worker() -> Any:\n            client = get_openai_client()\n            return client.embeddings.create(model=model, input=texts)\n        try:\n            resp = await asyncio.wait_for", "line_start": 81, "line_end": 115}, {"sha": "4452384401df59b088491976a34e11bed55425115c43b79bbe090a2f3ae41746", "index": 3, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\4452384401df59b088491976a34e11bed55425115c43b79bbe090a2f3ae41746.json", "terms": ["list", "fut", "texts", "model", "key", "not", "vec", "_inflight", "str", "continue", "for", "none", "out", "return", "inflight_waits", "set_result", "asyncio", "except", "exception", "float", "int", "items", "loop", "_cache_get", "_cache_put"], "text_preview": "except Exception:\n            pass\n    loop = asyncio.get_running_loop()\n    fut = loop.create_future()\n    _inflight[key] = fut\n    try:\n        vec = await _call_single(model, t)\n        _cache_put(model, t, vec)\n        fut.set_result(vec)\n        retur", "line_start": 116, "line_end": 158}, {"sha": "0120891205960686ffdad4168e9c89045e9811f0777635e67686a21168bd2e9d", "index": 4, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\0120891205960686ffdad4168e9c89045e9811f0777635e67686a21168bd2e9d.json", "terms": ["val", "for", "order", "dedup_map", "append", "key", "_inflight", "missing_vals", "idx", "vecs", "await", "batch", "list", "str", "futs_local", "missing_idx", "asyncio", "call", "dict", "fut", "inflight", "loop", "model", "not", "out"], "text_preview": "missing_idx.append(i)\n        missing_vals.append(t)\n\n    # Await inflight results\n    for i, fut in inflight_waits:\n        try:\n            res = await fut\n            out[i] = list(res or [])\n        except Exception:\n            out[i] = []\n\n    # Batc", "line_start": 159, "line_end": 191}, {"sha": "a70a3abab3cb5f70bf60dcdcb528468dab5c01d008014ce98b72b6ea9ab3e81b", "index": 5, "path": "4eda642b__jinx__micro__embeddings__embed_cache.py\\a70a3abab3cb5f70bf60dcdcb528468dab5c01d008014ce98b72b6ea9ab3e81b.json", "terms": ["val", "vec", "not", "get", "model", "none", "out", "pos", "_cache_put", "_inflight", "dedup_map", "futs_local", "set_result", "all", "and", "clear", "coalesced", "created", "done", "entry", "except", "exception", "fill", "for", "future"], "text_preview": "_cache_put(model, val, vec)\n            # resolve coalesced future if we created it here\n            f = futs_local.get(val)\n            if f is not None and not f.done():\n                try:\n                    f.set_result(vec)\n                except Ex", "line_start": 192, "line_end": 206}], "file_terms": ["model", "list", "str", "return", "vec", "none", "for", "not", "val", "float", "await", "fut", "asyncio", "text", "texts", "def", "except", "exception", "key", "out", "try", "_inflight", "import", "async", "from"]}