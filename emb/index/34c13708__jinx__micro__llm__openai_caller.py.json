{"file_rel": "jinx\\micro\\llm\\openai_caller.py", "file_sha256": "e91d606623d92d919ebc57e554ab08f66200231d2a0789f4a43854f29d44d446", "updated_ts": 1760763312.2847974, "total_chunks": 7, "chunks": [{"sha": "e7e0a2bde53f6dca7a56562577043e4ed7d628ee40e197902ce76b89a5e82f7c", "index": 0, "path": "34c13708__jinx__micro__llm__openai_caller.py\\e7e0a2bde53f6dca7a56562577043e4ed7d628ee40e197902ce76b89a5e82f7c.json", "terms": ["import", "from", "str", "jinx", "openai_api_key", "the", "api", "return", "bomb_log", "llm_disabled", "and", "asyncio", "call", "configured", "key", "micro", "model", "openai", "output", "stub", "text", "__future__", "_asyncio", "_is_code_like", "_queue"], "text_preview": "from __future__ import annotations\n\nimport asyncio\nimport os\nfrom typing import Any\n\nfrom jinx.logging_service import bomb_log\nfrom jinx.micro.rag.file_search import build_file_search_tools\nfrom jinx.net import get_openai_client\nfrom .llm_cache import call", "line_start": 1, "line_end": 30}, {"sha": "6f4156ebd8b226f5a7bc92303842455dfbe3faf00b4b593816d458fb138e7a6d", "index": 1, "path": "34c13708__jinx__micro__llm__openai_caller.py\\6f4156ebd8b226f5a7bc92303842455dfbe3faf00b4b593816d458fb138e7a6d.json", "terms": ["str", "extra_kwargs", "input_text", "fs_gate_on", "instructions", "model", "sample", "and", "await", "except", "exception", "not", "off", "path", "single", "try", "_is_code_like", "bomb_log", "build_file_search_tools", "call_openai_cached", "call_openai_validated", "code_id", "jinx_filesearch_gate", "jinx_llm_multi_enable", "any"], "text_preview": "# Heuristic: enable File Search tools only for code-like queries unless gated off\n        try:\n            fs_gate_on = (os.getenv(\"JINX_FILESEARCH_GATE\", \"1\").strip().lower() not in (\"\", \"0\", \"false\", \"off\", \"no\"))\n        except Exception:\n            fs", "line_start": 31, "line_end": 59}, {"sha": "5972b301fffa6bbe0a9eebfd77c7dcb32f1f193f522dc1d14ce8de27a045707e", "index": 2, "path": "34c13708__jinx__micro__llm__openai_caller.py\\5972b301fffa6bbe0a9eebfd77c7dcb32f1f193f522dc1d14ce8de27a045707e.json", "terms": ["input_text", "str", "extra_kwargs", "instructions", "model", "code_id", "fs_gate_on", "multi_on", "not", "off", "none", "await", "except", "exception", "false", "getenv", "lower", "return", "strip", "true", "_is_code_like", "base_extra_kwargs", "build_file_search_tools", "call_openai_cached", "call_openai_multi_validated"], "text_preview": "multi_on = (os.getenv(\"JINX_LLM_MULTI_ENABLE\", \"1\").strip().lower() not in (\"\", \"0\", \"false\", \"off\", \"no\"))\n    except Exception:\n        multi_on = True\n    # Heuristic: enable File Search tools only for code-like queries unless gated off\n    try:\n       ", "line_start": 60, "line_end": 96}, {"sha": "23d034e8c1c9d27fb98f95a47647dcda3a56a01fda388cf3acdddacbd4144b2f", "index": 3, "path": "34c13708__jinx__micro__llm__openai_caller.py\\23d034e8c1c9d27fb98f95a47647dcda3a56a01fda388cf3acdddacbd4144b2f.json", "terms": ["client", "responses", "stream", "code_id", "fs_gate_on", "python_", "none", "str", "extra_kwargs", "input_text", "stream_fn", "getattr", "instructions", "model", "any", "api", "false", "not", "streaming", "try", "_is_code_like", "_queue", "_worker", "build_file_search_tools", "get_openai_client"], "text_preview": "\"\"\"Stream Responses API, fire early when first complete <python_{code_id}> block appears.\n\n    Fallback to validated non-stream call on any streaming error.\n    \"\"\"\n    # File Search gating\n    try:\n        fs_gate_on = (os.getenv(\"JINX_FILESEARCH_GATE\", \"", "line_start": 97, "line_end": 126}, {"sha": "4408bec91ab93d2d3619f9e032bdddc077600adca1a5dff02d09054699863da0", "index": 4, "path": "34c13708__jinx__micro__llm__openai_caller.py\\4408bec91ab93d2d3619f9e032bdddc077600adca1a5dff02d09054699863da0.json", "terms": ["event", "getattr", "piece", "typ", "delta", "put", "str", "_asyncio", "_queue", "output_text", "queue", "endswith", "except", "exception", "for", "stream", "streaming", "text", "__done__", "__error__", "_get_next", "_worker", "create_task", "extra_kwargs", "to_thread"], "text_preview": "**{k: v for k, v in (extra_kwargs or {}).items() if not str(k).startswith(\"__\")},\n            ) as stream:\n                for event in stream:\n                    try:\n                        typ = getattr(event, \"type\", \"\") or \"\"\n                    exce", "line_start": 127, "line_end": 152}, {"sha": "2c4baa7332c5c247b808c60f8be1bafbcd1a234653281db1ed8f5ea091e8cb80", "index": 5, "path": "34c13708__jinx__micro__llm__openai_caller.py\\2c4baa7332c5c247b808c60f8be1bafbcd1a234653281db1ed8f5ea091e8cb80.json", "terms": ["chunk", "buf", "ltag", "text", "await", "body", "join", "try", "_asyncio", "on_first_block", "and", "except", "exception", "find", "fired", "len", "pass", "return", "true", "__done__", "__error__", "_get_next", "code_id", "create_task", "to_thread"], "text_preview": "return await _asyncio.to_thread(q.get)\n\n    try:\n        while True:\n            chunk = await _get_next()\n            if chunk == \"__DONE__\":\n                break\n            if isinstance(chunk, str) and chunk.startswith(\"__ERROR__:\"):\n                r", "line_start": 153, "line_end": 183}, {"sha": "b9dc60065319aa0d93ef9dceeb4c65efb00c2df5129e5687f87111ed980a93b3", "index": 6, "path": "34c13708__jinx__micro__llm__openai_caller.py\\b9dc60065319aa0d93ef9dceeb4c65efb00c2df5129e5687f87111ed980a93b3.json", "terms": ["code_id", "call_openai_validated", "input_text", "await", "call", "except", "exception", "fallback", "instructions", "model", "never", "outbound", "path", "return", "single", "started", "streaming", "validated"], "text_preview": "except Exception:\n        # Fallback to validated path (single outbound call if streaming never started)\n        return await call_openai_validated(instructions, model, input_text, code_id=code_id)", "line_start": 184, "line_end": 186}], "file_terms": ["str", "input_text", "extra_kwargs", "model", "instructions", "code_id", "import", "except", "exception", "not", "try", "fs_gate_on", "await", "return", "getattr", "event", "chunk", "and", "text", "from", "off", "piece", "responses", "stream", "true"]}