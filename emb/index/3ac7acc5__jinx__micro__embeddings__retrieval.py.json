{"file_rel": "jinx\\micro\\embeddings\\retrieval.py", "file_sha256": "4dd0f06870890408abb52eef98e1ab73e6cdba67d07fd9591c67f181b4b9c35e", "updated_ts": 1760763311.7842848, "total_chunks": 9, "chunks": [{"sha": "91a4840da20724b1e9eab8a2c83dbb000a5dbb24ba0ccfa6e7189098c8f623ab", "index": 0, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\91a4840da20724b1e9eab8a2c83dbb000a5dbb24ba0ccfa6e7189098c8f623ab.json", "terms": ["import", "getenv", "from", "int", "_hot_ttl_ms", "runtime", "str", "__future__", "default_top_k", "embed_cache", "embed_exhaustive", "embed_max_files_per_source", "embed_max_sources", "embed_min_preview_len", "embed_recency_window_sec", "embed_root", "embed_runtime_hot_ttl_ms", "embed_score_threshold", "embed_text_cached", "embed_top_k", "is_noise_text", "iter_items", "iter_recent_items", "max_files_per_source", "max_sources"], "text_preview": "from __future__ import annotations\n\nimport asyncio\nimport time\nimport os\nfrom typing import List, Tuple, Dict, Any\nimport hashlib\n\nfrom jinx.micro.embeddings.pipeline import iter_recent_items\nfrom .paths import EMBED_ROOT\nfrom .similarity import score_cosi", "line_start": 1, "line_end": 30}, {"sha": "3f34ac7091e6d42866287e88e731be21604926283301d021061fb72b6cb858f3", "index": 1, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\3f34ac7091e6d42866287e88e731be21604926283301d021061fb72b6cb858f3.json", "terms": ["str", "return", "def", "list", "k_eff", "none", "thr", "and", "any", "async", "dict", "qlen", "query", "tuple", "embed_root", "max_files_per_source", "max_sources", "scan_iter_items", "int", "await", "float", "max", "text", "_embed_query", "_iter_items"], "text_preview": "from .hot_store import get_runtime_items_hot\n\nasync def _load_runtime_items() -> List[Tuple[str, Dict[str, Any]]]:\n    return await asyncio.to_thread(scan_iter_items, EMBED_ROOT, MAX_FILES_PER_SOURCE, MAX_SOURCES)\n\n\nasync def _embed_query(text: str) -> Lis", "line_start": 31, "line_end": 60}, {"sha": "1b004b40dadb3ff9657bd3dc782c0fe263e7813cc2c17710cb3ad2683178f5ed", "index": 2, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\1b004b40dadb3ff9657bd3dc782c0fe263e7813cc2c17710cb3ad2683178f5ed.json", "terms": ["src_l", "time", "max", "meta", "float", "getenv", "k_eff", "create_task", "qv_task", "state_boost", "state_rec_mult", "str", "thr", "asyncio", "except", "exception", "first", "for", "get", "items", "lower", "obj", "query", "recent", "score"], "text_preview": "thr = max(0.2, thr)\n        k_eff = max(k_eff, 6)\n\n    # Overlap query embedding with a hot-store refresh to reduce wall time\n    qv_task = asyncio.create_task(_embed_query(query))\n    hot_task = asyncio.create_task(get_runtime_items_hot(_load_runtime_item", "line_start": 61, "line_end": 90}, {"sha": "5ee8bd02e41aabb98afd5bb5469807a08d5ef7ea53bc0818a6be6f74b9336560", "index": 3, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\5ee8bd02e41aabb98afd5bb5469807a08d5ef7ea53bc0818a6be6f74b9336560.json", "terms": ["meta", "get", "append", "len", "obj", "_recent_objs", "_recent_vecs", "k_eff", "rec", "score", "scored", "sim", "recency_window_sec", "_recent_meta", "age", "break", "continue", "max", "sims", "source", "strip", "is_noise_text", "min_preview_len", "score_cosine_batch", "short_q"], "text_preview": "pv = (meta.get(\"text_preview\") or \"\").strip()\n        if len(pv) < MIN_PREVIEW_LEN or is_noise_text(pv):\n            continue\n        _recent_objs.append(obj)\n        _recent_meta.append(meta)\n        _recent_vecs.append(obj.get(\"embedding\") or [])\n       ", "line_start": 91, "line_end": 115}, {"sha": "60b0ee33b3c34d85f5c6cb6e3456080e992f72122a9b32fb5fb5acda9710bbed", "index": 4, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\60b0ee33b3c34d85f5c6cb6e3456080e992f72122a9b32fb5fb5acda9710bbed.json", "terms": ["list", "scored", "buf_vecs", "str", "eff_budget", "buf_meta_src", "float", "none", "return", "k_eff", "meta_i", "obj_i", "the", "and", "any", "dict", "for", "get", "items", "key", "lambda", "not", "persisted", "reverse", "sim"], "text_preview": "scored.sort(key=lambda x: x[0], reverse=True)\n        return scored[:k_eff]\n    eff_budget = None if EXHAUSTIVE else max_time_ms\n    if eff_budget is not None and (time.perf_counter() - t0) * 1000.0 > eff_budget:\n        scored.sort(key=lambda x: x[0], rev", "line_start": 116, "line_end": 140}, {"sha": "2e45cb42499021b4dec1ca59bf725752986cfffe1c11d975dab9dd9a22145ea0", "index": 5, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\2e45cb42499021b4dec1ca59bf725752986cfffe1c11d975dab9dd9a22145ea0.json", "terms": ["meta_src_l", "src_l", "get", "strip", "meta", "state", "lower", "obj", "append", "idx", "rec", "score", "src", "recency_window_sec", "allow_src", "buf_vecs", "src_i", "continue", "dialogue", "len", "sandbox", "source", "startswith", "buf_meta_src", "meta_i"], "text_preview": "rec = 0.0 if RECENCY_WINDOW_SEC <= 0 else max(0.0, 1.0 - (age / RECENCY_WINDOW_SEC))\n            score = 0.8 * sim + 0.2 * rec\n            src_l = (src_i or \"\").strip().lower()\n            meta_src_l = (meta_i.get(\"source\") or \"\").strip().lower()\n         ", "line_start": 141, "line_end": 166}, {"sha": "e211c856847b9a3e8d69781eb4d0c506a787554b027a723e42c40f717c1c58d1", "index": 6, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\e211c856847b9a3e8d69781eb4d0c506a787554b027a723e42c40f717c1c58d1.json", "terms": ["none", "eff_budget", "buf_vecs", "set", "and", "str", "scored", "max_time_ms", "buf_meta_src", "k_eff", "int", "_flush", "perf_counter", "identical", "await", "break", "from", "hits", "idx", "len", "not", "query", "return", "time", "build_context_for"], "text_preview": "_flush(buf_vecs, buf_meta_src)\n            buf_vecs = []\n            buf_meta_src = []\n            if (idx % 50) == 49:\n                await asyncio.sleep(0)\n            if len(scored) >= k_eff:\n                break\n            if eff_budget is not None ", "line_start": 167, "line_end": 194}, {"sha": "3b0d2059f30c2123e6b8b071ad9116f2e6ce6c7222561c9b465476b76bf3652c", "index": 7, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\3b0d2059f30c2123e6b8b071ad9116f2e6ce6c7222561c9b465476b76bf3652c.json", "terms": ["csha", "get", "meta", "q_hash", "body_parts", "query", "continue", "for", "strip", "hits_sorted", "seen_hash", "and", "add", "not", "obj", "seen", "the", "content_sha256", "is_noise_text", "retrieve_top_k", "sha256", "text_preview", "append", "avoid", "breaks"], "text_preview": "q_hash = hashlib.sha256((query or \"\").strip().encode(\"utf-8\", errors=\"ignore\")).hexdigest() if query else \"\"\n    body_parts: List[str] = []\n    # Preserve chronological order in the final context to avoid semantic chaos.\n    # We first select by similarity", "line_start": 195, "line_end": 219}, {"sha": "bc3a8f7e6cd511270139354053c089816f0a91d95c69a0686366d8f0d7601b4f", "index": 8, "path": "3ac7acc5__jinx__micro__embeddings__retrieval.py\\bc3a8f7e6cd511270139354053c089816f0a91d95c69a0686366d8f0d7601b4f.json", "terms": ["embeddings_context", "body_parts", "body", "join", "return", "max_chars", "add", "and", "between", "blank", "break", "for", "hints", "inside", "line", "not", "padding", "readability", "tag", "the", "total", "with"], "text_preview": "if total > max_chars:\n            break\n\n    if not body_parts:\n        return \"\"\n\n    # Join with a blank line between hints for readability and add padding inside the tag\n    body = \"\\n\".join(body_parts)\n    return f\"<embeddings_context>\\n{body}\\n</embed", "line_start": 220, "line_end": 228}], "file_terms": ["str", "get", "meta", "and", "k_eff", "scored", "list", "import", "obj", "return", "int", "for", "getenv", "strip", "src_l", "none", "buf_vecs", "query", "float", "from", "len", "max", "time", "continue", "lower"]}