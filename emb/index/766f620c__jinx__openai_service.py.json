{"file_rel": "jinx\\openai_service.py", "file_sha256": "5fed470f029afe05a468c7e9ef60bcc50a67889b09326ad51d8f703675fff4d0", "updated_ts": 1760763309.7560635, "total_chunks": 1, "chunks": [{"sha": "25f52ab7bf4a5cac17fc3f6cd52b1a0df7e5d55e7579d3692b93affe7b443d4c", "index": 0, "path": "766f620c__jinx__openai_service.py\\25f52ab7bf4a5cac17fc3f6cd52b1a0df7e5d55e7579d3692b93affe7b443d4c.json", "terms": ["code_primer", "spark_openai", "micro", "service", "from", "import", "jinx", "llm", "the", "__all__", "__future__", "annotations", "api", "delegating", "facade", "implementation", "keep", "module", "openai", "public", "stable", "thin", "under", "wrapper"], "text_preview": "\"\"\"OpenAI service facade.\n\nThin wrapper delegating to the micro-module implementation under\n``jinx.micro.llm.service`` to keep the public API stable.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom jinx.micro.llm.service import (\n    code_primer as code_prim", "line_start": 1, "line_end": 18}], "file_terms": ["code_primer", "spark_openai", "micro", "service", "from", "import", "jinx", "llm", "the", "__all__", "__future__", "annotations", "api", "delegating", "facade", "implementation", "keep", "module", "openai", "public", "stable", "thin", "under", "wrapper"]}