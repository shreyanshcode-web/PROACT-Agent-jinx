{"file_rel": "jinx\\micro\\embeddings\\project_terms.py", "file_sha256": "987e3b53cda926842f00a269b3aabfe388158288f266273de2861101e62258d7", "updated_ts": 1760763311.6372724, "total_chunks": 3, "chunks": [{"sha": "ca95a0ee2f0cb9039fbb87998d09386a504368e7bc7ffc97756ddcaccb3e78f7", "index": 0, "path": "d4b2259f__jinx__micro__embeddings__project_terms.py\\ca95a0ee2f0cb9039fbb87998d09386a504368e7bc7ffc97756ddcaccb3e78f7.json", "terms": ["import", "_fn", "extract_terms", "top_k", "str", "for", "list", "out", "token", "_mod", "jinx_terms_plugin", "line_occurrence_ratio", "digits", "from", "importlib", "plugin", "score", "text", "the", "tokens", "with", "__future__", "import_module", "agnostic", "and"], "text_preview": "from __future__ import annotations\n\nimport re\nimport importlib\nfrom typing import List\n\n\ndef extract_terms(text: str, top_k: int = 25) -> List[str]:\n    \"\"\"Language-agnostic term extractor without stopwords.\n\n    Approach:\n    - Tokenize with Unicode-aware", "line_start": 1, "line_end": 30}, {"sha": "f6ec8fff5268d28c0e358057691921c6d7c403f02f731e1ed6c924661bc222b0", "index": 1, "path": "d4b2259f__jinx__micro__embeddings__project_terms.py\\f6ec8fff5268d28c0e358057691921c6d7c403f02f731e1ed6c924661bc222b0.json", "terms": ["for", "line_occ", "text", "not", "str", "non_empty_lines", "seen_in_line", "w_raw", "get", "line", "lines", "total_lines", "word_re", "set", "add", "continue", "dict", "float", "int", "len", "return", "strip", "score_of", "alphabetic", "and"], "text_preview": "text = text or \"\"\n    if not text.strip():\n        return []\n\n    # Split into lines for line-occurrence stats\n    lines = text.splitlines()\n    non_empty_lines = [ln for ln in lines if ln.strip()]\n    total_lines = max(1, len(non_empty_lines))\n\n    # Coll", "line_start": 31, "line_end": 68}, {"sha": "3c8a27e6c7ee13ddf7e90177380aec27f20caea302d25c09fa5e1ee07414bf86", "index": 2, "path": "d4b2259f__jinx__micro__embeddings__project_terms.py\\3c8a27e6c7ee13ddf7e90177380aec27f20caea302d25c09fa5e1ee07414bf86.json", "terms": ["items", "for", "base", "return", "score_of", "top_k", "any", "bonus", "code", "contexts", "digits", "filter", "float", "get", "identifier", "indicate", "isdigit", "key", "keys", "lambda", "like", "max", "near", "rank", "scores"], "text_preview": "base = float(tf.get(w, 0)) * max(0.0, 1.0 - lf)\n        # Identifier-like bonus: underscores or digits indicate specificity in code contexts\n        if (\"_\" in w) or any(ch.isdigit() for ch in w):\n            base *= 1.1\n        return base\n\n    # Rank tok", "line_start": 69, "line_end": 79}], "file_terms": ["for", "str", "text", "items", "return", "line_occ", "top_k", "get", "import", "line", "lines", "not", "_fn", "extract_terms", "non_empty_lines", "seen_in_line", "w_raw", "base", "bonus", "digits", "float", "identifier", "int", "like", "list"]}